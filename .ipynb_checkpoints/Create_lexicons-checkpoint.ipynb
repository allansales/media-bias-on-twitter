{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testar nao remover hashtags, remover stopwords\n",
    "\n",
    "## Usar o vocabulario que foi formado para calcular pearson como entrada para calcular o BoW e o TF-iDF e fazer classificação de texto."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "tweets = pd.read_csv('data/tweets.csv', encoding=\"UTF-8\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove undesirable values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "tweets = tweets[tweets.userOrientation.isin([\"target\", \"left\", \"right\"])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "target_tweets = tweets[tweets.userOrientation == \"target\"]\n",
    "tweets = tweets[tweets.userOrientation != \"target\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# lower case\n",
    "text = tweets.text.str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# tokenize words\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "tokenizer = TweetTokenizer()\n",
    "\n",
    "lists_tokens = text.apply(tokenizer.tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# remove punctuation\n",
    "new_lists_tokens = list()\n",
    "for list_tokens in lists_tokens:\n",
    "    new_list_tokens = list()\n",
    "    for token in list_tokens:\n",
    "        if token.isalpha():\n",
    "            new_list_tokens.append(token)\n",
    "    new_lists_tokens.append(new_list_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# lemmatize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "lemmatized_lists = list()\n",
    "for list_tokens in new_lists_tokens:\n",
    "    words_lemma = [lemmatizer.lemmatize(token) for token in list_tokens]\n",
    "    lemmatized_lists.append(words_lemma)\n",
    "    \n",
    "lemmatized_series = pd.Series(lemmatized_lists)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 2gram and 3gram calculation\n",
    "from nltk import ngrams\n",
    "\n",
    "def get_ngrams(ngram_size, to_extend):\n",
    "    \n",
    "    ngrams_list = to_extend.apply(ngrams, args=(ngram_size,))\n",
    "    new_n_list = []\n",
    "    for grams in ngrams_list:\n",
    "        tuple_gram = [gram for gram in grams]\n",
    "        string_tuple = ['_'.join(n_tuple) for n_tuple in tuple_gram]\n",
    "        new_n_list.append(string_tuple)\n",
    "    return new_n_list\n",
    "\n",
    "bigrams = get_ngrams(2, lemmatized_series)\n",
    "trigrams = get_ngrams(3, lemmatized_series)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# add 2 gram and 3 gram t\n",
    "def list_extend(lst, item):\n",
    "    lst.extend(item)\n",
    "    return lst\n",
    "\n",
    "lemmatized_series = [list_extend(lemma, bigram) for lemma, bigram in zip(lemmatized_series, bigrams)]\n",
    "lemmatized_series = [list_extend(lemma, trigram) for lemma, trigram in zip(lemmatized_series, trigrams)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_phrases_from_list(list_of_lists):\n",
    "    phrase_list = [' '.join(list_of_words) for list_of_words in list_of_lists]\n",
    "    return phrase_list\n",
    "\n",
    "rebuid_tweets = get_phrases_from_list(lemmatized_series)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_vocabulary(list_of_lists):\n",
    "    flat_list = [word for list_of_words in list_of_lists for word in list_of_words]\n",
    "    flat_list = set(flat_list)\n",
    "    return flat_list\n",
    "\n",
    "vocab = get_vocabulary(lemmatized_series)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Paper implementation - What Drives Media Slant?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "phrases_orientation = pd.DataFrame({'features' : lemmatized_series, 'orientation' : tweets.userOrientation})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def reshape_dataframe(row):\n",
    "    return pd.DataFrame({'word':row['features'], 'orientation':row['orientation']})\n",
    "\n",
    "list_of_words_orientation = []\n",
    "for index, row in phrases_orientation.iterrows():\n",
    "    new_rows = reshape_dataframe(row)\n",
    "    list_of_words_orientation.append(new_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "words_orientation = pd.concat(list_of_words_orientation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Phrase lenght"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "phrases_length = [string.count('_')+1 for string in words_orientation.word]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "words_orientation['length'] = phrases_length"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pearson Statistic for every phrase"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Legend:\n",
    "\n",
    "p = phrase\n",
    "\n",
    "l = length of phrase p\n",
    "\n",
    "o = orientation\n",
    "\n",
    "e = tweeted by a left-wing supporter\n",
    "\n",
    "d = tweeted by a right-wing supporter\n",
    "\n",
    "Ex:\n",
    "Fple = frequency of a l-length phrase wrote by an profile biased towards the left-wing\n",
    "\n",
    "fnple = frequency of l-length phrases except of p, wrote by an profile biased towards the left-wing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "frequency_plo = words_orientation.groupby(words_orientation.columns.tolist(), as_index=False).size()\n",
    "frequency_plo = frequency_plo.to_frame(\"frequency\").reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Discard phrases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def discard_phrases_by_length_freq(table, length, min_freq, max_freq):\n",
    "    table = table.drop(table[(table.length == length) & (table.frequency < min_freq)].index)\n",
    "    table = table.drop(table[(table.length == length) & (table.frequency > max_freq)].index)\n",
    "    return table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "frequency_plo = discard_phrases_by_length_freq(frequency_plo, 1, 100, 5000)\n",
    "frequency_plo = discard_phrases_by_length_freq(frequency_plo, 2, 50, 500)\n",
    "frequency_plo = discard_phrases_by_length_freq(frequency_plo, 3, 10, 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "##### Pearson Statistic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def pearson_statistic(target_word):\n",
    "    \n",
    "    # frequency of phrase p of length l for left and right tweets\n",
    "    pled = frequency_plo[frequency_plo.word == target_word]\n",
    "    \n",
    "    # length of phrase p\n",
    "    if len(pled.index) == 0:\n",
    "        return \"-\"\n",
    "    \n",
    "    length = pled.length.iloc[0]\n",
    "    \n",
    "    if(\"left\" not in pled.orientation.unique()):\n",
    "        complement = pd.DataFrame({\"orientation\":\"left\", \"word\":target_word, \"length\":length, \"frequency\":0}, index = [0])\n",
    "        pled = pled.append(complement)\n",
    "        \n",
    "    if(\"right\" not in pled.orientation.unique()):\n",
    "        complement = pd.DataFrame({\"orientation\": \"right\", \"word\":target_word, \"length\":length, \"frequency\":0}, index = [0])\n",
    "        pled = pled.append(complement)\n",
    "\n",
    "    # frequency of not phrase p of length l for left and right tweets\n",
    "    not_pled = frequency_plo[(frequency_plo.word != target_word) & (frequency_plo.length == length)]\n",
    "    not_pled = not_pled.groupby(\"orientation\").frequency.sum().reset_index()\n",
    "    \n",
    "    pld = float(pled[pled.orientation == \"right\"].frequency)\n",
    "    ple = float(pled[pled.orientation == \"left\"].frequency)\n",
    "\n",
    "    not_pld = float(not_pled[not_pled.orientation == \"right\"].frequency)\n",
    "    not_ple = float(not_pled[not_pled.orientation == \"left\"].frequency)\n",
    "\n",
    "    X = (pld*not_ple - ple*not_pld)**2/((pld+ple)*(pld+not_pld)*(ple+not_ple)*(not_ple+not_pld))\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X = frequency_plo.word.apply(pearson_statistic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "word_pearson = pd.DataFrame({'word':frequency_plo.word, 'orientation':frequency_plo.orientation, 'X':X})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>X</th>\n",
       "      <th>orientation</th>\n",
       "      <th>word</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1429071</th>\n",
       "      <td>7.085834e-03</td>\n",
       "      <td>right</td>\n",
       "      <td>por</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1602889</th>\n",
       "      <td>7.054320e-03</td>\n",
       "      <td>right</td>\n",
       "      <td>uma</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1524580</th>\n",
       "      <td>6.916672e-03</td>\n",
       "      <td>right</td>\n",
       "      <td>se</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1549098</th>\n",
       "      <td>2.028756e-03</td>\n",
       "      <td>right</td>\n",
       "      <td>sobre_a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1103705</th>\n",
       "      <td>1.979482e-03</td>\n",
       "      <td>right</td>\n",
       "      <td>do_que</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1121484</th>\n",
       "      <td>1.934319e-03</td>\n",
       "      <td>right</td>\n",
       "      <td>e_não</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54811</th>\n",
       "      <td>1.795365e-03</td>\n",
       "      <td>left</td>\n",
       "      <td>ao_vivo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1018971</th>\n",
       "      <td>1.675731e-03</td>\n",
       "      <td>right</td>\n",
       "      <td>contra_o</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1352263</th>\n",
       "      <td>1.667524e-03</td>\n",
       "      <td>right</td>\n",
       "      <td>não_tem</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1062636</th>\n",
       "      <td>1.622387e-03</td>\n",
       "      <td>right</td>\n",
       "      <td>de_uma</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1343573</th>\n",
       "      <td>1.569048e-03</td>\n",
       "      <td>right</td>\n",
       "      <td>novo_brasil</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>715705</th>\n",
       "      <td>1.560168e-03</td>\n",
       "      <td>left</td>\n",
       "      <td>rt_a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>886442</th>\n",
       "      <td>1.519817e-03</td>\n",
       "      <td>right</td>\n",
       "      <td>a_gente</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1255525</th>\n",
       "      <td>1.503408e-03</td>\n",
       "      <td>right</td>\n",
       "      <td>jovem_pan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>916315</th>\n",
       "      <td>1.444216e-03</td>\n",
       "      <td>right</td>\n",
       "      <td>alvaro</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>801555</th>\n",
       "      <td>1.440806e-03</td>\n",
       "      <td>left</td>\n",
       "      <td>todos_o</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>531137</th>\n",
       "      <td>1.415123e-03</td>\n",
       "      <td>left</td>\n",
       "      <td>novo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1343427</th>\n",
       "      <td>1.415123e-03</td>\n",
       "      <td>right</td>\n",
       "      <td>novo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1018212</th>\n",
       "      <td>1.368050e-03</td>\n",
       "      <td>right</td>\n",
       "      <td>contra_a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1367615</th>\n",
       "      <td>1.327040e-03</td>\n",
       "      <td>right</td>\n",
       "      <td>o_povo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1373667</th>\n",
       "      <td>1.318935e-03</td>\n",
       "      <td>right</td>\n",
       "      <td>obrigado</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>564110</th>\n",
       "      <td>1.318935e-03</td>\n",
       "      <td>left</td>\n",
       "      <td>obrigado</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1314974</th>\n",
       "      <td>1.286032e-03</td>\n",
       "      <td>right</td>\n",
       "      <td>muito_obrigado</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1366711</th>\n",
       "      <td>1.269630e-03</td>\n",
       "      <td>right</td>\n",
       "      <td>o_país</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1189894</th>\n",
       "      <td>1.199554e-03</td>\n",
       "      <td>right</td>\n",
       "      <td>fernando_haddad</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>353802</th>\n",
       "      <td>1.199554e-03</td>\n",
       "      <td>left</td>\n",
       "      <td>fernando_haddad</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>352878</th>\n",
       "      <td>1.097356e-03</td>\n",
       "      <td>left</td>\n",
       "      <td>feliz_de</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1271888</th>\n",
       "      <td>1.096333e-03</td>\n",
       "      <td>right</td>\n",
       "      <td>lobão</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1254340</th>\n",
       "      <td>1.081042e-03</td>\n",
       "      <td>right</td>\n",
       "      <td>jornal_da</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>205275</th>\n",
       "      <td>1.043150e-03</td>\n",
       "      <td>left</td>\n",
       "      <td>de_temer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>296844</th>\n",
       "      <td>2.970369e-10</td>\n",
       "      <td>left</td>\n",
       "      <td>em_mais_de</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1180001</th>\n",
       "      <td>2.970369e-10</td>\n",
       "      <td>right</td>\n",
       "      <td>fake_news_que</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1581399</th>\n",
       "      <td>2.970369e-10</td>\n",
       "      <td>right</td>\n",
       "      <td>tipo_de_gente</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>886495</th>\n",
       "      <td>2.970369e-10</td>\n",
       "      <td>right</td>\n",
       "      <td>a_gente_fala</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1394863</th>\n",
       "      <td>2.970369e-10</td>\n",
       "      <td>right</td>\n",
       "      <td>para_são_paulo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>775255</th>\n",
       "      <td>2.970369e-10</td>\n",
       "      <td>left</td>\n",
       "      <td>são_paulo_em</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1139271</th>\n",
       "      <td>2.970369e-10</td>\n",
       "      <td>right</td>\n",
       "      <td>em_mais_de</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1035846</th>\n",
       "      <td>2.970369e-10</td>\n",
       "      <td>right</td>\n",
       "      <td>da_falta_de</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1645751</th>\n",
       "      <td>2.970369e-10</td>\n",
       "      <td>right</td>\n",
       "      <td>é_o_de</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>668307</th>\n",
       "      <td>2.970369e-10</td>\n",
       "      <td>left</td>\n",
       "      <td>que_a_polícia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>612470</th>\n",
       "      <td>2.970369e-10</td>\n",
       "      <td>left</td>\n",
       "      <td>pior_do_que</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1353611</th>\n",
       "      <td>2.970369e-10</td>\n",
       "      <td>right</td>\n",
       "      <td>não_é_à</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1598354</th>\n",
       "      <td>2.970369e-10</td>\n",
       "      <td>right</td>\n",
       "      <td>um_cara_que</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8175</th>\n",
       "      <td>2.970369e-10</td>\n",
       "      <td>left</td>\n",
       "      <td>a_gente_fala</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1467430</th>\n",
       "      <td>2.970369e-10</td>\n",
       "      <td>right</td>\n",
       "      <td>que_a_polícia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>584857</th>\n",
       "      <td>2.970369e-10</td>\n",
       "      <td>left</td>\n",
       "      <td>para_participar_do</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>797863</th>\n",
       "      <td>2.970369e-10</td>\n",
       "      <td>left</td>\n",
       "      <td>tipo_de_gente</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1563679</th>\n",
       "      <td>2.970369e-10</td>\n",
       "      <td>right</td>\n",
       "      <td>são_paulo_em</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>818484</th>\n",
       "      <td>2.970369e-10</td>\n",
       "      <td>left</td>\n",
       "      <td>um_cara_que</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>542172</th>\n",
       "      <td>2.970369e-10</td>\n",
       "      <td>left</td>\n",
       "      <td>não_é_à</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1393659</th>\n",
       "      <td>2.970369e-10</td>\n",
       "      <td>right</td>\n",
       "      <td>para_participar_do</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>175028</th>\n",
       "      <td>2.970369e-10</td>\n",
       "      <td>left</td>\n",
       "      <td>da_falta_de</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1288407</th>\n",
       "      <td>2.668730e-10</td>\n",
       "      <td>right</td>\n",
       "      <td>manter</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>464910</th>\n",
       "      <td>2.668730e-10</td>\n",
       "      <td>left</td>\n",
       "      <td>manter</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1201692</th>\n",
       "      <td>1.774217e-10</td>\n",
       "      <td>right</td>\n",
       "      <td>fosse</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>367516</th>\n",
       "      <td>1.774217e-10</td>\n",
       "      <td>left</td>\n",
       "      <td>fosse</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>365471</th>\n",
       "      <td>6.952044e-12</td>\n",
       "      <td>left</td>\n",
       "      <td>forma_de</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1199878</th>\n",
       "      <td>6.952044e-12</td>\n",
       "      <td>right</td>\n",
       "      <td>forma_de</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1454856</th>\n",
       "      <td>2.855121e-14</td>\n",
       "      <td>right</td>\n",
       "      <td>prova</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>654772</th>\n",
       "      <td>2.855121e-14</td>\n",
       "      <td>left</td>\n",
       "      <td>prova</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10861 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                    X orientation                word\n",
       "1429071  7.085834e-03       right                 por\n",
       "1602889  7.054320e-03       right                 uma\n",
       "1524580  6.916672e-03       right                  se\n",
       "1549098  2.028756e-03       right             sobre_a\n",
       "1103705  1.979482e-03       right              do_que\n",
       "1121484  1.934319e-03       right               e_não\n",
       "54811    1.795365e-03        left             ao_vivo\n",
       "1018971  1.675731e-03       right            contra_o\n",
       "1352263  1.667524e-03       right             não_tem\n",
       "1062636  1.622387e-03       right              de_uma\n",
       "1343573  1.569048e-03       right         novo_brasil\n",
       "715705   1.560168e-03        left                rt_a\n",
       "886442   1.519817e-03       right             a_gente\n",
       "1255525  1.503408e-03       right           jovem_pan\n",
       "916315   1.444216e-03       right              alvaro\n",
       "801555   1.440806e-03        left             todos_o\n",
       "531137   1.415123e-03        left                novo\n",
       "1343427  1.415123e-03       right                novo\n",
       "1018212  1.368050e-03       right            contra_a\n",
       "1367615  1.327040e-03       right              o_povo\n",
       "1373667  1.318935e-03       right            obrigado\n",
       "564110   1.318935e-03        left            obrigado\n",
       "1314974  1.286032e-03       right      muito_obrigado\n",
       "1366711  1.269630e-03       right              o_país\n",
       "1189894  1.199554e-03       right     fernando_haddad\n",
       "353802   1.199554e-03        left     fernando_haddad\n",
       "352878   1.097356e-03        left            feliz_de\n",
       "1271888  1.096333e-03       right               lobão\n",
       "1254340  1.081042e-03       right           jornal_da\n",
       "205275   1.043150e-03        left            de_temer\n",
       "...               ...         ...                 ...\n",
       "296844   2.970369e-10        left          em_mais_de\n",
       "1180001  2.970369e-10       right       fake_news_que\n",
       "1581399  2.970369e-10       right       tipo_de_gente\n",
       "886495   2.970369e-10       right        a_gente_fala\n",
       "1394863  2.970369e-10       right      para_são_paulo\n",
       "775255   2.970369e-10        left        são_paulo_em\n",
       "1139271  2.970369e-10       right          em_mais_de\n",
       "1035846  2.970369e-10       right         da_falta_de\n",
       "1645751  2.970369e-10       right              é_o_de\n",
       "668307   2.970369e-10        left       que_a_polícia\n",
       "612470   2.970369e-10        left         pior_do_que\n",
       "1353611  2.970369e-10       right             não_é_à\n",
       "1598354  2.970369e-10       right         um_cara_que\n",
       "8175     2.970369e-10        left        a_gente_fala\n",
       "1467430  2.970369e-10       right       que_a_polícia\n",
       "584857   2.970369e-10        left  para_participar_do\n",
       "797863   2.970369e-10        left       tipo_de_gente\n",
       "1563679  2.970369e-10       right        são_paulo_em\n",
       "818484   2.970369e-10        left         um_cara_que\n",
       "542172   2.970369e-10        left             não_é_à\n",
       "1393659  2.970369e-10       right  para_participar_do\n",
       "175028   2.970369e-10        left         da_falta_de\n",
       "1288407  2.668730e-10       right              manter\n",
       "464910   2.668730e-10        left              manter\n",
       "1201692  1.774217e-10       right               fosse\n",
       "367516   1.774217e-10        left               fosse\n",
       "365471   6.952044e-12        left            forma_de\n",
       "1199878  6.952044e-12       right            forma_de\n",
       "1454856  2.855121e-14       right               prova\n",
       "654772   2.855121e-14        left               prova\n",
       "\n",
       "[10861 rows x 3 columns]"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_pearson.sort_values(['X'], ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BoW "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "countvec = CountVectorizer(vocabulary=vocab)\n",
    "bow_tweets = countvec.fit_transform(rebuid_tweets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tdidfvec = TfidfVectorizer(vocabulary=vocab)\n",
    "tfidf_tweets = tdidfvec.fit_transform(rebuid_tweets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BOW Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(bow_tweets, tweets.userOrientation, test_size=0.25, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "scoring = ['precision_macro', 'recall_macro', 'f1_macro', 'accuracy']\n",
    "#clf = DecisionTreeClassifier(random_state=0)\n",
    "#scores = cross_validate(clf, X_train, y_train, scoring=scoring, cv=5, return_train_score=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model persistance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from joblib import dump, load\n",
    "\n",
    "#dump(clf, 'text_orientation_classifier.joblib') \n",
    "clf = load('text_orientation_classifier.joblib')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "prediction = clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "names = ['left', 'right']\n",
    "\n",
    "print(classification_report(y_test, prediction, names))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get most important features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "feature_importance = pd.DataFrame({'features' : list(vocab), 'importances' : clf.feature_importances_})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "feature_importance[feature_importance.importances != 0.0].sort_values(by=['importances'], ascending=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## TF-IDF Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(tfidf_tweets, tweets.userOrientation, test_size=0.25, random_state=40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "scoring = ['precision_macro', 'recall_macro', 'f1_macro', 'accuracy']\n",
    "clf_tfidf = DecisionTreeClassifier(random_state=0)\n",
    "#scores = cross_validate(clf, X_train, y_train, scoring=scoring, cv=5, return_train_score=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "clf_tfidf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "prediction_tfidf = clf_tfidf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "names = ['left', 'right']\n",
    "\n",
    "print(classification_report(y_test, prediction_tfidf, names))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get most important features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "feature_importance_tfidf = pd.DataFrame({'features' : list(vocab), 'importances' : clf_tfidf.feature_importances_})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "feature_importance_tfidf[feature_importance_tfidf.importances != 0.0].sort_values(by=['importances'], ascending=False)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
